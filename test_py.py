# -*- coding: utf-8 -*-
"""Test.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ctf1L5Wk-ksZwyA1Ob0vARUqtPMwjETH
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/')

!pip install numpy pandas matplotlib opencv-python scikit-learn tensorflow

# src/data_loader.py
# -*- coding: utf-8 -*-
"""
Script để tải, tiền xử lý và tạo pipeline dữ liệu cho mô hình segmentation/matting.
"""
import os
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf # Sử dụng tf.data cho pipeline hiệu quả

# --- Các hằng số và cấu hình ---
# Kích thước ảnh đầu vào cho mô hình. Kích thước lớn hơn thường cho kết quả tốt hơn
# nhưng yêu cầu nhiều tài nguyên hơn (GPU RAM).
IMG_WIDTH = 512
IMG_HEIGHT = 512
IMG_CHANNELS = 3 # Số kênh màu của ảnh gốc (ví dụ: RGB)

# !!! QUAN TRỌNG: Thay đổi đường dẫn này cho phù hợp với máy của bạn !!!
# Ví dụ: 'D:/BaiTapLon/DancingPeopleSegmentation/data/'
# Hoặc: '/home/user/projects/DancingPeopleSegmentation/data/'
BASE_DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/segmentation_full_body_tik_tok_2615_img/segmentation_full_body_tik_tok_2615_img' # Vui lòng cập nhật đường dẫn này

IMAGE_DIR = os.path.join(BASE_DATA_PATH, 'images')
MASK_DIR = os.path.join(BASE_DATA_PATH, 'masks')

def load_image_and_mask_paths(image_dir, mask_dir):
    """
    Lấy danh sách đường dẫn đầy đủ đến các file ảnh và mask tương ứng.
    Hàm này giả định tên file trong 'images' và 'masks' là giống nhau.
    """
    if not os.path.isdir(image_dir):
        print(f"Lỗi: Thư mục ảnh không tồn tại: {image_dir}")
        return [], []
    if not os.path.isdir(mask_dir):
        print(f"Lỗi: Thư mục mask không tồn tại: {mask_dir}")
        return [], []
    image_filenames = sorted(os.listdir(image_dir))
    mask_filenames = sorted(os.listdir(mask_dir)) # Giả sử tên file tương ứng

    # Tạo đường dẫn đầy đủ
    image_paths = [os.path.join(image_dir, fname) for fname in image_filenames]
    mask_paths = [os.path.join(mask_dir, fname) for fname in mask_filenames]

    # Chỉ giữ lại các cặp file mà cả ảnh và mask đều tồn tại
    # và có tên cơ sở giống nhau (quan trọng để đảm bảo khớp ảnh-mask)
    valid_image_paths = []
    valid_mask_paths = []

    # Tạo một set các tên file mask để tìm kiếm nhanh hơn
    mask_basename_set = {os.path.basename(p) for p in mask_paths}

    for img_path in image_paths:
        img_basename = os.path.basename(img_path)
        # Giả định tên file ảnh và mask là giống nhau.
        # Nếu mask có hậu tố khác (ví dụ: _mask.png), bạn cần điều chỉnh logic ở đây.
        # Ví dụ: mask_basename = img_basename.replace('.jpg', '_mask.png')
        if img_basename in mask_basename_set:
            mask_path = os.path.join(mask_dir, img_basename)
            if os.path.exists(img_path) and os.path.exists(mask_path):
                valid_image_paths.append(img_path)
                valid_mask_paths.append(mask_path)
        else:
            print(f"Cảnh báo: Không tìm thấy mask tương ứng cho ảnh: {img_path}")


    if not valid_image_paths:
         print(f"Không có cặp ảnh/mask hợp lệ nào được tìm thấy. Vui lòng kiểm tra đường dẫn và tên file.")
         return [], [] # Trả về rỗng nếu không có dữ liệu

    print(f"Tìm thấy {len(valid_image_paths)} cặp ảnh/mask hợp lệ.")
    return valid_image_paths, valid_mask_paths


def preprocess_image(image_path, target_height=IMG_HEIGHT, target_width=IMG_WIDTH):
    """
    Đọc, resize và chuẩn hóa ảnh gốc.
    """
    img = cv.imread(image_path, cv.IMREAD_COLOR) # Đọc ảnh màu
    if img is None:
        print(f"Lỗi: Không thể đọc ảnh: {image_path}")
        return None
    img = cv.cvtColor(img, cv.COLOR_BGR2RGB) # Chuyển sang RGB (Matplotlib/TF thường dùng RGB)
    img = cv.resize(img, (target_width, target_height), interpolation=cv.INTER_AREA)
    img = img / 255.0 # Chuẩn hóa giá trị pixel về [0, 1]
    return img.astype(np.float32)

def preprocess_mask(mask_path, target_height=IMG_HEIGHT, target_width=IMG_WIDTH):
    """
    Đọc, resize và chuẩn hóa ảnh mask.
    QUAN TRỌNG: Nếu bạn làm matting (tách nền với độ trong suốt như tóc),
    mask phải là ảnh grayscale với giá trị từ 0-255 (alpha matte).
    Nếu mask chỉ là nhị phân (đen/trắng), bạn đang làm segmentation.
    Để có kết quả matting tốt, mask ground truth nên có các giá trị bán trong suốt.
    """
    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE) # Đọc ảnh mask dưới dạng xám
    if mask is None:
        print(f"Lỗi: Không thể đọc mask: {mask_path}")
        return None

    # Đối với Matting (alpha matte), sử dụng INTER_LINEAR hoặc INTER_CUBIC để giữ các giá trị trung gian
    # Đối với Segmentation (mask nhị phân), INTER_NEAREST là phù hợp
    mask = cv.resize(mask, (target_width, target_height), interpolation=cv.INTER_LINEAR)

    # Chuẩn hóa giá trị pixel về [0, 1]
    # Nếu mask là alpha matte (0-255), chia cho 255.0
    mask = mask / 255.0

    # Nếu mask của bạn chỉ là nhị phân (ví dụ: từ segmentation), bạn có thể nhị phân hóa lại:
    # mask = (mask > 0.5).astype(np.float32) # Ngưỡng 0.5 sau khi chia 255.0

    mask = np.expand_dims(mask, axis=-1) # Thêm chiều kênh: (H, W, 1)
    return mask.astype(np.float32)

def tf_dataset_generator(image_paths, mask_paths, batch_size, shuffle=True, augment=False):
    """
    Tạo một tf.data.Dataset để tải và tiền xử lý dữ liệu theo batch,
    có hỗ trợ data augmentation.
    """
    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))

    if shuffle:
        buffer_size = len(image_paths)
        dataset = dataset.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)

    def load_and_preprocess(img_path_tensor, mask_path_tensor):
        def _py_load_and_preprocess(img_path, mask_path):
            img_p = img_path.numpy().decode('utf-8')
            mask_p = mask_path.numpy().decode('utf-8')

            img = preprocess_image(img_p)
            mask = preprocess_mask(mask_p)

            if img is None or mask is None:
                print(f"Cảnh báo: Lỗi khi tải {img_p} hoặc {mask_p}. Trả về mảng zeros.")
                return np.zeros((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32), \
                       np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)
            return img, mask

        image, mask = tf.py_function(
            _py_load_and_preprocess,
            [img_path_tensor, mask_path_tensor],
            [tf.float32, tf.float32]
        )
        image.set_shape([IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])
        mask.set_shape([IMG_HEIGHT, IMG_WIDTH, 1])
        return image, mask

    # Áp dụng bước tải và tiền xử lý cơ bản
    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)

    # --- Thêm bước Data Augmentation ---
    if augment:
        def apply_augmentation(image, mask):
            # Lật ngang ngẫu nhiên
            if tf.random.uniform(()) > 0.5:
                image = tf.image.flip_left_right(image)
                mask = tf.image.flip_left_right(mask)

            # Thay đổi độ sáng ngẫu nhiên
            image = tf.image.random_brightness(image, max_delta=0.1) # Điều chỉnh max_delta tùy ý

            # Thay đổi độ tương phản ngẫu nhiên
            image = tf.image.random_contrast(image, lower=0.8, upper=1.2) # Điều chỉnh lower/upper tùy ý

            # Đảm bảo giá trị pixel của ảnh vẫn trong khoảng [0, 1] sau các biến đổi
            image = tf.clip_by_value(image, 0.0, 1.0)

            # Lưu ý: Các phép xoay (rotation) hoặc dịch chuyển (translation) phức tạp hơn
            # cần được xử lý cẩn thận để đảm bảo mask và ảnh vẫn đồng bộ.
            # Bạn có thể dùng tf.keras.layers.RandomRotation/RandomTranslation
            # nhưng chúng cần được gọi trong một hàm tf.py_function hoặc Lambda layer
            # nếu bạn muốn áp dụng chúng ở đây trong pipeline tf.data.
            # Với segmentation, đảm bảo mask biến đổi y hệt ảnh.

            return image, mask

        # Áp dụng augmentation chỉ khi augment=True
        dataset = dataset.map(apply_augmentation, num_parallel_calls=tf.data.AUTOTUNE)

    # Tạo batch
    dataset = dataset.batch(batch_size)

    # Prefetch để tối ưu hiệu năng
    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)

    return dataset

def get_datasets(test_size=0.2, batch_size=32, random_state=42):
    all_image_paths, all_mask_paths = load_image_and_mask_paths(IMAGE_DIR, MASK_DIR)

    if not all_image_paths or not all_mask_paths:
        print("Không có dữ liệu để xử lý. Kết thúc.")
        return None, None, 0, 0

    train_img_paths, val_img_paths, train_mask_paths, val_mask_paths = train_test_split(
        all_image_paths, all_mask_paths,
        test_size=test_size,
        random_state=random_state
    )

    print(f"Số lượng mẫu huấn luyện: {len(train_img_paths)}")
    print(f"Số lượng mẫu kiểm định: {len(val_img_paths)}")

    # Truyền tham số augment=True cho tập huấn luyện
    train_dataset = tf_dataset_generator(train_img_paths, train_mask_paths, batch_size, shuffle=True, augment=True)
    # Không augment cho tập validation
    val_dataset = tf_dataset_generator(val_img_paths, val_mask_paths, batch_size, shuffle=False, augment=False)

    return train_dataset, val_dataset, len(train_img_paths), len(val_img_paths)

# --- Ví dụ cách sử dụng (có thể chạy riêng file này để kiểm tra) ---
if __name__ == '__main__':
    print("Đang kiểm tra data_loader...")

    if 'D:/ZangDev/CodeByZang/Phenikaa/ComputerVision/Project/data/' in BASE_DATA_PATH:
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print("!!! LƯU Ý: Bạn CHƯA CẬP NHẬT `BASE_DATA_PATH` trong src/data_loader.py !!!")
        print("!!! Vui lòng mở file src/data_loader.py và cập nhật đường dẫn này cho đúng dữ liệu của bạn. !!!")
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        # Không exit() để người dùng vẫn có thể chạy thử nếu đường dẫn không đúng nhưng muốn xem cấu trúc

    BATCH_SIZE_TEST = 4
    train_ds, val_ds, num_train_samples, num_val_samples = get_datasets(batch_size=BATCH_SIZE_TEST)

    if train_ds and val_ds:
        print(f"\nĐã tạo thành công train_dataset và val_dataset.")
        print(f"Số mẫu huấn luyện: {num_train_samples}, Số mẫu kiểm định: {num_val_samples}")

        print("\nKiểm tra một batch từ train_dataset:")
        for images, masks in train_ds.take(1):
            print(f"  Shape của batch ảnh: {images.shape}")
            print(f"  Shape của batch mask: {masks.shape}")
            print(f"  Kiểu dữ liệu ảnh: {images.dtype}")
            print(f"  Kiểu dữ liệu mask: {masks.dtype}")
            print(f"  Giá trị min/max của ảnh trong batch: {np.min(images.numpy()):.2f} / {np.max(images.numpy()):.2f}")
            print(f"  Giá trị min/max của mask trong batch: {np.min(masks.numpy()):.2f} / {np.max(masks.numpy()):.2f}") # Thay đổi .0f thành .2f để xem giá trị alpha
            print(f"  Các giá trị duy nhất trong một mask mẫu: {np.unique(masks.numpy()[0])}")


            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.imshow(images.numpy()[0])
            plt.title("Ảnh mẫu từ batch")
            plt.axis('off')

            plt.subplot(1, 2, 2)
            plt.imshow(masks.numpy()[0][:, :, 0], cmap='gray', vmin=0, vmax=1) # Đặt vmin/vmax để hiển thị đúng alpha
            plt.title("Mask mẫu từ batch (Alpha Matte)")
            plt.axis('off')
            plt.show()
            break
    else:
        print("Không thể tạo datasets. Vui lòng kiểm tra lỗi ở trên.")

# src/model.py
# -*- coding: utf-8 -*-
"""
Script định nghĩa kiến trúc mô hình U-Net cho bài toán semantic segmentation/matting.
"""
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate
from tensorflow.keras.models import Model

# --- Các hằng số (có thể import từ data_loader.py hoặc định nghĩa lại nếu cần) ---
# Giả sử chúng ta sẽ truyền input_shape vào hàm build_unet
NUM_CLASSES = 1 # Output là mask nhị phân (0 hoặc 1 cho chủ thể/nền) hoặc alpha matte (giá trị 0-1)

def conv_block(input_tensor, num_filters, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', padding='same'):
    """
    Một khối convolutional cơ bản gồm 2 lớp Conv2D.
    """
    # Lớp Convolutional thứ nhất
    x = Conv2D(num_filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding)(input_tensor)
    x = BatchNormalization()(x)
    x = Activation(activation)(x)
    # Lớp Convolutional thứ hai
    x = Conv2D(num_filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding)(x)
    x = BatchNormalization()(x)
    x = Activation(activation)(x)
    return x

def encoder_block(input_tensor, num_filters, pool_size=(2, 2), dropout_rate=0.1):
    """
    Một khối trong phần Encoder của U-Net.
    Bao gồm một conv_block, một lớp Dropout (tùy chọn), và một lớp MaxPooling.
    """
    c = conv_block(input_tensor, num_filters)
    if dropout_rate > 0:
        c = Dropout(dropout_rate)(c)
    p = MaxPooling2D(pool_size)(c)
    return c, p # Trả về cả output của conv_block (cho skip connection) và output của pooling

def decoder_block(input_tensor, skip_features, num_filters, kernel_size=(3,3), strides=(2,2), padding='same', dropout_rate=0.1):
    """
    Một khối trong phần Decoder của U-Net.
    Bao gồm một lớp Conv2DTranspose (upsampling), nối với skip_features,
    và một conv_block.
    """
    up = Conv2DTranspose(num_filters, kernel_size, strides=strides, padding=padding)(input_tensor)
    # Nối (concatenate) với đặc trưng từ skip connection
    # Đảm bảo skip_features có cùng kích thước không gian với 'up'
    # Nếu không, bạn có thể cần Cropping2D hoặc padding trên skip_features
    merged = concatenate([up, skip_features])
    c = conv_block(merged, num_filters)
    if dropout_rate > 0:
        c = Dropout(dropout_rate)(c)
    return c

def build_unet(input_shape, num_classes=NUM_CLASSES):
    """
    Xây dựng kiến trúc mô hình U-Net.

    Args:
        input_shape (tuple): Shape của ảnh đầu vào, ví dụ (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS).
        num_classes (int): Số lớp đầu ra. Cho binary segmentation (chủ thể/nền) hoặc alpha matte, num_classes=1.

    Returns:
        tensorflow.keras.models.Model: Mô hình U-Net đã được xây dựng.
    """
    inputs = Input(input_shape)

    # --- Encoder (Contracting Path / Đường xuống) ---
    c1, p1 = encoder_block(inputs, num_filters=32, dropout_rate=0.1)
    c2, p2 = encoder_block(p1, num_filters=64, dropout_rate=0.1)
    c3, p3 = encoder_block(p2, num_filters=128, dropout_rate=0.2)
    c4, p4 = encoder_block(p3, num_filters=256, dropout_rate=0.2)

    # --- Bottleneck (Phần cổ chai) ---
    b = conv_block(p4, num_filters=512)
    if 0.3 > 0: # Giữ dropout_rate nhất quán
        b = Dropout(0.3)(b)


    # --- Decoder (Expansive Path / Đường lên) ---
    d1 = decoder_block(b, skip_features=c4, num_filters=256, dropout_rate=0.2)
    d2 = decoder_block(d1, skip_features=c3, num_filters=128, dropout_rate=0.2)
    d3 = decoder_block(d2, skip_features=c2, num_filters=64, dropout_rate=0.1)
    d4 = decoder_block(d3, skip_features=c1, num_filters=32, dropout_rate=0.1)

    # --- Output Layer ---
    # Sử dụng Conv2D với kernel_size (1,1) và hàm kích hoạt sigmoid cho binary segmentation hoặc alpha matte.
    outputs = Conv2D(num_classes, (1, 1), activation='sigmoid')(d4)

    # Tạo mô hình
    model = Model(inputs=[inputs], outputs=[outputs], name="UNet_DancerSegmentation")
    return model

# --- Ví dụ cách sử dụng (có thể chạy riêng file này để kiểm tra kiến trúc) ---
if __name__ == '__main__':
    IMG_HEIGHT_TEST = 128
    IMG_WIDTH_TEST = 128
    IMG_CHANNELS_TEST = 3 # Ảnh màu RGB

    example_input_shape = (IMG_HEIGHT_TEST, IMG_WIDTH_TEST, IMG_CHANNELS_TEST)

    print(f"Đang xây dựng mô hình U-Net với input_shape: {example_input_shape}")

    unet_model = build_unet(input_shape=example_input_shape)

    print("\nKiến trúc mô hình U-Net:")
    unet_model.summary()

    # (Tùy chọn) Vẽ kiến trúc mô hình ra file ảnh (cần cài đặt pydot và graphviz)
    # try:
    #     tf.keras.utils.plot_model(unet_model, to_file='unet_model_architecture.png', show_shapes=True)
    #     print("\nĐã lưu kiến trúc mô hình vào file 'unet_model_architecture.png'")
    # except ImportError:
    #     print("\nKhông thể vẽ kiến trúc mô hình. Cần cài đặt pydot và graphviz.")
    #     print("Lệnh cài đặt: pip install pydot graphviz")

import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import os
import numpy as np # Dùng để kiểm tra giá trị min/max của alpha matte

# Import các phần đã viết
#from model import build_unet
#from data_loader import get_datasets, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS # Import các hằng số từ data_loader

# --- Định nghĩa các Hàm Loss Tối Ưu ---

def dice_loss(y_true, y_pred, smooth=1e-6):
    """
    Dice Loss cho bài toán segmentation/matting.
    Tối ưu hóa trực tiếp sự chồng lấn giữa ground truth và dự đoán.
    """
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return 1 - ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))

def bce_loss(y_true, y_pred):
    """
    Binary Cross-Entropy Loss.
    Sử dụng from_logits=False vì đầu ra của UNet đã qua sigmoid.
    """
    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)
    return bce(y_true, y_pred)

def bce_dice_loss(y_true, y_pred):
    """
    Hàm Loss kết hợp Binary Cross-Entropy và Dice Loss.
    Đây là sự kết hợp mạnh mẽ và thường được khuyến nghị cho segmentation/matting.
    Bạn có thể điều chỉnh trọng số giữa BCE và Dice nếu muốn (ví dụ: 0.5 * bce_loss(...) + 0.5 * dice_loss(...)).
    """
    return bce_loss(y_true, y_pred) + dice_loss(y_true, y_pred) * 0.7
    #return bce_loss(y_true, y_pred) + dice_loss(y_true, y_pred)

# --- Cấu hình huấn luyện ---
# Sử dụng AUTOTUNE để tối ưu buffer size
BUFFER_SIZE = tf.data.AUTOTUNE
# Kích thước ảnh (lấy từ data_loader)
IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)
# Số kênh màu (lấy từ data_loader)
CHANNELS = IMG_CHANNELS
# Kích thước batch. Có thể tăng nếu GPU RAM cho phép để tăng tốc huấn luyện.
BATCH_SIZE = 4
# Learning rate ban đầu. Có thể điều chỉnh tùy thuộc vào kết quả thử nghiệm.
LEARNING_RATE = 1e-4
# Tổng số epoch bạn muốn chạy (có thể dừng sớm với EarlyStopping)
EPOCHS = 50

# Số lớp đầu ra (1 cho alpha matte hoặc binary mask)
NUM_CLASSES = 1

# Thư mục để lưu checkpoint mô hình
CHECKPOINT_DIR = './checkpoints'
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

def train_model():
    print("Đang tải dữ liệu...")
    # Sử dụng hàm get_datasets đã định nghĩa trong data_loader.py
    train_ds, val_ds, num_train_samples, num_val_samples = get_datasets(
        test_size=0.2, # Tỷ lệ dữ liệu dùng làm validation
        batch_size=BATCH_SIZE,
        random_state=42
    )

    if train_ds is None or val_ds is None:
        print("Không có dữ liệu để huấn luyện hoặc kiểm định. Vui lòng kiểm tra lại đường dẫn và dữ liệu.")
        return

    print(f"\nBắt đầu huấn luyện với {num_train_samples} mẫu train và {num_val_samples} mẫu validation.")
    print(f"Kích thước ảnh đầu vào: {IMG_SIZE[0]}x{IMG_SIZE[1]}x{CHANNELS}")
    input_shape = (IMG_SIZE[0], IMG_SIZE[1], CHANNELS)

    print("Đang xây dựng mô hình UNet...")
    # Sử dụng hàm build_unet đã định nghĩa trong model.py
    model = build_unet(input_shape=input_shape, num_classes=NUM_CLASSES)

    print("Đang biên dịch mô hình...")
    # Optimizer: Adam là lựa chọn phổ biến
    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)

    # Loss Function: Sử dụng hàm loss kết hợp BCE + Dice Loss
    # Đây là thay đổi quan trọng nhất để cải thiện mean_io_u
    loss_fn = bce_dice_loss

    # Metrics:
    # tf.keras.metrics.BinaryAccuracy()
    # tf.keras.metrics.MeanIoU(num_classes=2) là metric quan trọng cho segmentation/matting.
    metrics = [
        tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),
        tf.keras.metrics.MeanIoU(num_classes=2, name='mean_io_u') # Đổi tên cho dễ đọc
    ]

    model.compile(optimizer=optimizer,
                    loss=loss_fn,
                    metrics=metrics)

    # --- Callbacks ---
    # ModelCheckpoint: Lưu lại mô hình tốt nhất dựa trên mean_io_u trên tập validation
    # Lưu toàn bộ mô hình (cả kiến trúc và weights)
    model_checkpoint = ModelCheckpoint(
        filepath=os.path.join(CHECKPOINT_DIR, 'unet_best_model.keras'),
        save_weights_only=False,
        monitor='val_mean_io_u', # Theo dõi MeanIoU trên tập validation
        mode='max', # Mode 'max' vì ta muốn MeanIoU cao nhất
        save_best_only=True, # Chỉ lưu khi metric tốt hơn trước đó
        verbose=1 # Hiển thị thông báo khi lưu
    )

    # EarlyStopping: Dừng huấn luyện sớm nếu metric validation không cải thiện
    early_stopping = EarlyStopping(
        monitor='val_mean_io_u', # Theo dõi MeanIoU trên tập validation
        patience=10, # Chờ 10 epoch không cải thiện trước khi dừng
        mode='max', # Mode 'max' vì ta muốn MeanIoU cao nhất
        restore_best_weights=True, # Khôi phục lại trọng số của epoch tốt nhất
        verbose=1
    )

    # ReduceLROnPlateau: Giảm learning rate khi metric validation không cải thiện
    reduce_lr = ReduceLROnPlateau(
        monitor='val_mean_io_u', # Theo dõi MeanIoU trên tập validation
        factor=0.5, # Giảm learning rate xuống còn 50%
        patience=5, # Chờ 5 epoch không cải thiện
        min_lr=1e-7, # Learning rate tối thiểu
        mode='max', # Mode 'max' vì ta muốn MeanIoU cao nhất
        verbose=1
    )

    # List các callbacks sẽ sử dụng
    callbacks_list = [early_stopping, model_checkpoint, reduce_lr]

    # --- Huấn luyện mô hình ---
    print("\nĐang huấn luyện mô hình...")
    history = model.fit(
        train_ds, # Dataset huấn luyện
        epochs=EPOCHS, # Tổng số epoch
        validation_data=val_ds, # Dataset kiểm định
        callbacks=callbacks_list # Áp dụng các callbacks đã định nghĩa
    )

    print("\nHuấn luyện hoàn thành.")

    # --- Vẽ biểu đồ kết quả huấn luyện (Loss và Metrics) ---
    print("\nVẽ biểu đồ kết quả huấn luyện...")
    history_dict = history.history

    loss_values = history_dict['loss']
    val_loss_values = history_dict['val_loss']

    binary_accuracy_values = history_dict['binary_accuracy']
    val_binary_accuracy_values = history_dict['val_binary_accuracy']

    mean_iou_values = history_dict['mean_io_u']
    val_mean_iou_values = history_dict['val_mean_io_u']

    epochs_range = range(1, len(loss_values) + 1)

    plt.figure(figsize=(18, 6)) # Tăng kích thước figure để dễ nhìn hơn

    # Biểu đồ Loss
    plt.subplot(1, 3, 1)
    plt.plot(epochs_range, loss_values, 'bo-', label='Training Loss') # Thêm 'o-' để dễ nhìn điểm
    plt.plot(epochs_range, val_loss_values, 'b-', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Biểu đồ Binary Accuracy
    plt.subplot(1, 3, 2)
    plt.plot(epochs_range, binary_accuracy_values, 'ro-', label='Training Accuracy')
    plt.plot(epochs_range, val_binary_accuracy_values, 'r-', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Biểu đồ Mean IoU
    plt.subplot(1, 3, 3)
    plt.plot(epochs_range, mean_iou_values, 'go-', label='Training Mean IoU')
    plt.plot(epochs_range, val_mean_iou_values, 'g-', label='Validation Mean IoU')
    plt.title('Training and Validation Mean IoU')
    plt.xlabel('Epochs')
    plt.ylabel('Mean IoU')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    train_model()